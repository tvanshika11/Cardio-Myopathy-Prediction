{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CqX3XyFi6IF",
        "outputId": "08fd01bd-1c96-4f1e-bd52-c1972e7317dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JGGRN5j6ZJgP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import string\n",
        "import random\n",
        "import sys\n",
        "import unidecode\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "all_characters = string.printable       ## this is just to parse our characters from text file\n",
        "n_characters = len(all_characters)\n",
        "\n",
        "file = unidecode.unidecode(open(\"names.txt\").read()) ## insert every time we run this code\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        ## we are creating num_layers layers for RNN's for our predictions\n",
        "        ## each step is going to take vector of size hidden_size\n",
        "        ## input_size is size of input vector\n",
        "        ## output size is required output vector size\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embed = nn.Embedding(input_size, hidden_size) ## convert input->hiddle_size\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True) ## input = hidden_size and each vector size == hidden_size\n",
        "        self.fc = nn.Linear(hidden_size, output_size) ## final fully connected layer to convert finalVector to output_size vector\n",
        "\n",
        "    def forward(self, x, hidden, cell):\n",
        "        out = self.embed(x)\n",
        "        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n",
        "        out = self.fc(out.reshape(out.shape[0], -1))\n",
        "        return out, (hidden, cell)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        return hidden, cell\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## this is going to generate text/characters in desired format  and use defined RNN layer to work accordingly\n",
        "class Generator:\n",
        "    def __init__(self):\n",
        "        self.chunk_len = 250\n",
        "        self.num_epochs = 5000\n",
        "        self.batch_size = 1\n",
        "        self.print_every = 50\n",
        "        self.hidden_size = 256\n",
        "        self.num_layers = 2\n",
        "        self.lr = 0.003\n",
        "\n",
        "    def char_tensor(self, string):\n",
        "        ## inserting index of each character at it's position\n",
        "        tensor = torch.zeros(len(string)).long()\n",
        "        for c in range(len(string)):\n",
        "            tensor[c] = all_characters.index(string[c])\n",
        "        return tensor\n",
        "      ## getting random batch by selecting some random startId\n",
        "    def get_random_batch(self):\n",
        "        start_idx = random.randint(0, len(file) - self.chunk_len)\n",
        "        end_idx = start_idx + self.chunk_len + 1\n",
        "        text_str = file[start_idx:end_idx]\n",
        "        text_input = torch.zeros(self.batch_size, self.chunk_len)\n",
        "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            ## input doesnot have end token\n",
        "            text_input[i, :] = self.char_tensor(text_str[:-1])\n",
        "            ## output does not have start token\n",
        "            text_target[i, :] = self.char_tensor(text_str[1:])\n",
        "        return text_input.long(), text_target.long()\n",
        "\n",
        "    def generate(self, initial_str=\"A\", predict_len=100, temperature=0.85):\n",
        "        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
        "        initial_input = self.char_tensor(initial_str)\n",
        "        predicted = initial_str\n",
        "\n",
        "        for p in range(len(initial_str) - 1):\n",
        "            _, (hidden, cell) = self.rnn(\n",
        "                initial_input[p].view(1).to(device), hidden, cell\n",
        "            )\n",
        "\n",
        "        last_char = initial_input[-1]\n",
        "\n",
        "        for p in range(predict_len):\n",
        "            output, (hidden, cell) = self.rnn(\n",
        "                last_char.view(1).to(device), hidden, cell\n",
        "            )\n",
        "            output_dist = output.data.view(-1).div(temperature).exp()\n",
        "            top_char = torch.multinomial(output_dist, 1)[0]\n",
        "            predicted_char = all_characters[top_char]\n",
        "            predicted += predicted_char\n",
        "            last_char = self.char_tensor(predicted_char)\n",
        "\n",
        "        return predicted\n",
        "\n",
        "    # input_size, hidden_size, num_layers, output_size\n",
        "    def train(self):\n",
        "        self.rnn = RNN(\n",
        "            n_characters, self.hidden_size, self.num_layers, n_characters\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        writer = SummaryWriter(f\"runs/testing\")  # for tensorboard\n",
        "\n",
        "        print(\"=> Starting training\")\n",
        "\n",
        "        for epoch in range(1, self.num_epochs + 1):\n",
        "            inp, target = self.get_random_batch()\n",
        "            hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
        "\n",
        "            self.rnn.zero_grad()\n",
        "            loss = 0\n",
        "            inp = inp.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            for c in range(self.chunk_len):\n",
        "                output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n",
        "                loss += criterion(output, target[:, c])\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss = loss.item() / self.chunk_len\n",
        "\n",
        "            if epoch % self.print_every == 0:\n",
        "                print(f\"Loss: {loss}\")\n",
        "                print(self.generate())\n",
        "\n",
        "            writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n"
      ],
      "metadata": {
        "id": "Gj7_X-_yZPNp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## setup for tensorbaord\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "x4kZNTGepAdn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gennames = Generator()\n",
        "gennames.train()"
      ],
      "metadata": {
        "id": "BHsd_lr5ZrZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee5d2763-32b0-4271-c9c4-6ca12f5b53b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Starting training\n",
            "Loss: 2.269477294921875\n",
            "Amqie\n",
            "Manmei\n",
            "Joreie\n",
            "Acildorie\n",
            "Arie\n",
            "fonriesy\n",
            "Janie\n",
            "Aall\n",
            "Jamish\n",
            "Selhsa\n",
            "ienhisok\n",
            "Joriha\n",
            "Erere\n",
            "Qelhania\n",
            "C\n",
            "Loss: 2.196859619140625\n",
            "Ah\n",
            "Aldin\n",
            "Derr\n",
            "Oless\n",
            "Clalir\n",
            "Glinig\n",
            "Brelie\n",
            "Morla\n",
            "Bettern\n",
            "Lohy\n",
            "Raco\n",
            "Kakece\n",
            "Jerin\n",
            "Anciey\n",
            "Jacbon\n",
            "Slon\n",
            "Lasi\n",
            "Loss: 2.2196630859375\n",
            "Aynh\n",
            "Charlie\n",
            "Maronaa\n",
            "Lentia\n",
            "Riltha\n",
            "Anduer\n",
            "Jesth\n",
            "Annati\n",
            "Buishe\n",
            "Jober\n",
            "Joveston\n",
            "kadie\n",
            "Lema\n",
            "BaLrathisn\n",
            "Fr\n",
            "Loss: 1.97085546875\n",
            "An\n",
            "Jaie\n",
            "Mardia\n",
            "Shedtie\n",
            "Alaie\n",
            "Cayla\n",
            "Harlica\n",
            "Daserania\n",
            "Keeette\n",
            "Kenney\n",
            "Bonelina\n",
            "Jela\n",
            "Malamey\n",
            "Roforatha\n",
            "L\n",
            "Loss: 2.119510498046875\n",
            "Aan\n",
            "Eubrii\n",
            "Kaylea\n",
            "Kailua\n",
            "Kardie\n",
            "Malad\n",
            "Jincia\n",
            "Eylira\n",
            "Ludy\n",
            "Cellie\n",
            "Vamian\n",
            "Lasa\n",
            "Prianna\n",
            "Edicon\n",
            "Kay\n",
            "Barict\n",
            "Loss: 1.8180987548828125\n",
            "Alen\n",
            "Shestie\n",
            "Mardana\n",
            "Jeanna\n",
            "Lacee\n",
            "Seltine\n",
            "Mikandra\n",
            "Dyllinah\n",
            "Luie\n",
            "Sue\n",
            "Ramary\n",
            "Ben\n",
            "Marimon\n",
            "Boima\n",
            "Nandon\n",
            "\n",
            "Loss: 1.75592236328125\n",
            "Aliace\n",
            "Jady\n",
            "Shacei\n",
            "Katrie\n",
            "Bonnie\n",
            "Chelly\n",
            "Idena\n",
            "Carana\n",
            "Adueline\n",
            "Evena\n",
            "Carvara\n",
            "Kaylee\n",
            "Hacie\n",
            "Carley\n",
            "Dann\n",
            "\n",
            "Loss: 1.793961669921875\n",
            "Arann\n",
            "Aveny\n",
            "Jighan\n",
            "Kalis\n",
            "Tenah\n",
            "Tuber\n",
            "Dennie\n",
            "Covey\n",
            "Jobis\n",
            "Dazahle\n",
            "Trobbicch\n",
            "Cara\n",
            "Ichel\n",
            "Krarre\n",
            "Danizalia\n",
            "Loss: 1.884855224609375\n",
            "Alyn\n",
            "Lyleia\n",
            "Garrice\n",
            "Dennetta\n",
            "Lacie\n",
            "Joneth\n",
            "Daven\n",
            "Grordan\n",
            "Dan\n",
            "Dever\n",
            "Lirma\n",
            "Alicha\n",
            "Katheria\n",
            "Colsa\n",
            "Mheanne\n",
            "Loss: 1.692\n",
            "Allin\n",
            "Arberta\n",
            "Rosie\n",
            "Flecca\n",
            "Andrey\n",
            "Bredda\n",
            "Alila\n",
            "Charli\n",
            "Allarel\n",
            "Annie\n",
            "Charey\n",
            "Tauda\n",
            "Negie\n",
            "Allegis\n",
            "Colpy\n",
            "\n",
            "Loss: 1.532576416015625\n",
            "Alian\n",
            "Dian\n",
            "Rhennie\n",
            "Kathler\n",
            "Molanor\n",
            "Jackel\n",
            "Bracis\n",
            "Machy\n",
            "Abricm\n",
            "Kella\n",
            "Rose\n",
            "Amera\n",
            "Andalke\n",
            "Suele\n",
            "Corris\n",
            "G\n",
            "Loss: 1.644876953125\n",
            "Alia\n",
            "Shela\n",
            "Lyngela\n",
            "Glanda\n",
            "Terry\n",
            "Garrell\n",
            "Karseph\n",
            "Hadel\n",
            "Dalla\n",
            "Salias\n",
            "Ebbenge\n",
            "Roe\n",
            "Lette\n",
            "Lynce\n",
            "Barrie\n",
            "Lai\n",
            "Loss: 1.6425509033203125\n",
            "Andra\n",
            "Bouslyn\n",
            "Lakaine\n",
            "Sherri\n",
            "Katherine\n",
            "Bracy\n",
            "Nellia\n",
            "Erice\n",
            "Derace\n",
            "Colly\n",
            "Rusa\n",
            "Maria\n",
            "Luelia\n",
            "Trawn\n",
            "Kelly\n",
            "\n",
            "Loss: 1.6160693359375\n",
            "Anna\n",
            "Joine\n",
            "Bennice\n",
            "Jillie\n",
            "Eliane\n",
            "Tina\n",
            "Yay\n",
            "Marine\n",
            "Jalise\n",
            "Jeannester\n",
            "Jachell\n",
            "Terrie\n",
            "Brannes\n",
            "Donnie\n",
            "Heri\n",
            "Loss: 1.7747669677734375\n",
            "Ashvia\n",
            "Dania\n",
            "Alengel\n",
            "Mark\n",
            "Gaylen\n",
            "Marie\n",
            "Loria\n",
            "Lenione\n",
            "Burisa\n",
            "Suzandra\n",
            "Vickie\n",
            "Nogan\n",
            "Maussa\n",
            "Shawna\n",
            "Conni\n",
            "Loss: 1.6374615478515624\n",
            "Aythan\n",
            "Christopher\n",
            "Dean\n",
            "Jon\n",
            "Lill\n",
            "Halyssa\n",
            "Haylyn\n",
            "Dances\n",
            "Sel\n",
            "Christy\n",
            "Estel\n",
            "Flondal\n",
            "Hamel\n",
            "Brandon\n",
            "Nanda\n",
            "\n",
            "Loss: 1.879776611328125\n",
            "Aurentet\n",
            "Dara\n",
            "Albert\n",
            "Garrell\n",
            "Garold\n",
            "Johnny\n",
            "Julian\n",
            "Marl\n",
            "Monick\n",
            "Jimmie\n",
            "Leouglard\n",
            "Janny\n",
            "Jornald\n",
            "Jannie\n",
            "M\n",
            "Loss: 1.3760294189453126\n",
            "Alla\n",
            "Mallard\n",
            "Lewis\n",
            "Betryna\n",
            "Annah\n",
            "Andre\n",
            "Brace\n",
            "Caster\n",
            "Dayla\n",
            "Susan\n",
            "Brancea\n",
            "Brace\n",
            "Garrell\n",
            "Judie\n",
            "Gord\n",
            "Ashl\n",
            "Loss: 1.478033935546875\n",
            "Alba\n",
            "Adrisha\n",
            "Annton\n",
            "Darry\n",
            "Candy\n",
            "Dian\n",
            "Cerry\n",
            "Olah\n",
            "Dennrellia\n",
            "Arold\n",
            "Walkam\n",
            "Benny\n",
            "Brysta\n",
            "Tony\n",
            "Patrine\n",
            "Bob\n",
            "Loss: 1.7033665771484374\n",
            "Alely\n",
            "Ervert\n",
            "Elvia\n",
            "Luz\n",
            "Marc\n",
            "Jone\n",
            "Marlene\n",
            "Melvan\n",
            "Rosie\n",
            "Ann\n",
            "Ervert\n",
            "Ivan\n",
            "Catrina\n",
            "Beth\n",
            "Arthel\n",
            "Harman\n",
            "Dyan\n",
            "Loss: 1.4079794921875\n",
            "Annya\n",
            "Ami\n",
            "Ellen\n",
            "Angel\n",
            "Samenzie\n",
            "Ebbinia\n",
            "Helma\n",
            "Larry\n",
            "Steve\n",
            "Ruoh\n",
            "Alance\n",
            "Sidney\n",
            "Willian\n",
            "Auguster\n",
            "Brence\n",
            "W\n",
            "Loss: 1.6171290283203126\n",
            "Almigias\n",
            "Edworina\n",
            "Lancy\n",
            "Natimaida\n",
            "Ashlee\n",
            "Alvin\n",
            "Stephew\n",
            "Delean\n",
            "Cornelia\n",
            "Eleigh\n",
            "Jonnie\n",
            "Kriston\n",
            "Leliad\n",
            "E\n",
            "Loss: 1.53379248046875\n",
            "Andra\n",
            "Kayla\n",
            "Vade\n",
            "Alejandre\n",
            "Garriel\n",
            "Ronda\n",
            "Renece\n",
            "Uynnon\n",
            "Adeline\n",
            "Bryant\n",
            "Dannie\n",
            "Angelos\n",
            "Connie\n",
            "Iustie\n",
            "Ma\n",
            "Loss: 1.4490081787109375\n",
            "Angeline\n",
            "Rachel\n",
            "Debbert\n",
            "Corine\n",
            "Holand\n",
            "Freddond\n",
            "ela\n",
            "Norma\n",
            "Johnnie\n",
            "Mytta\n",
            "Garrell\n",
            "Jasper\n",
            "Jeanne\n",
            "Deviniqu\n",
            "Loss: 1.29114111328125\n",
            "Athenia\n",
            "Samuel\n",
            "Astell\n",
            "Stepila\n",
            "Alfree\n",
            "Denny\n",
            "Philip\n",
            "Juanita\n",
            "Micholette\n",
            "Kindy\n",
            "Norma\n",
            "Cone\n",
            "Kobbin\n",
            "Kallie\n",
            "M\n",
            "Loss: 1.4930225830078125\n",
            "Arnion\n",
            "Cornette\n",
            "Augor\n",
            "Dolores\n",
            "Brady\n",
            "Spenna\n",
            "Emma\n",
            "Jayloi\n",
            "Hatty\n",
            "Nanela\n",
            "Tonia\n",
            "Ashlee\n",
            "Chestin\n",
            "Walter\n",
            "Audre\n",
            "Loss: 1.3819598388671874\n",
            "Anna\n",
            "Eryn\n",
            "Florody\n",
            "Jacken\n",
            "Philly\n",
            "Verrick\n",
            "Britte\n",
            "Carold\n",
            "Darrell\n",
            "Jacken\n",
            "Will\n",
            "Audrey\n",
            "Clared\n",
            "Elphia\n",
            "Tillia\n",
            "Loss: 1.4181812744140625\n",
            "Alivian\n",
            "Ellison\n",
            "Fwencia\n",
            "Mila\n",
            "Jessie\n",
            "Kloy\n",
            "Reba\n",
            "Sally\n",
            "Tiylara\n",
            "Carolyn\n",
            "Betty\n",
            "Celefanda\n",
            "Cassandra\n",
            "Anna\n",
            "Ti\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## loading board\n",
        "!tensorboard serve --logdir runs/testing"
      ],
      "metadata": {
        "id": "5wNA7-AipOWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlwZpWQ2otZ1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}