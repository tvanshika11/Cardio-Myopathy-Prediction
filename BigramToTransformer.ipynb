{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "oGpWjRLROZpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6Ng23OTJ30u"
      },
      "outputs": [],
      "source": [
        "dataset = \"\"\"\n",
        "Sure! Here's a 400-word essay on the topic of \"The Impact of Social Media on Society\":\n",
        "\n",
        "The Impact of Social Media on Society\n",
        "\n",
        "Social media has emerged as a powerful force in shaping the way we communicate, share information, and interact with others. With its rapid growth and widespread adoption, it has significantly impacted various aspects of society. While it has brought about numerous benefits, such as increased connectivity and access to information, social media has also raised concerns about privacy, mental health, and the spread of misinformation.\n",
        "\n",
        "One of the most significant impacts of social media is its ability to connect people from different parts of the world. Platforms like Facebook, Twitter, and Instagram have enabled individuals to form new relationships, reconnect with old friends, and build online communities. This connectivity has fostered a sense of global citizenship and facilitated the exchange of ideas and cultural diversity.\n",
        "\n",
        "Moreover, social media has revolutionized the way information is disseminated. News spreads rapidly on platforms like Twitter, making it a valuable tool for staying updated on current events. It has given a voice to marginalized communities, allowing them to share their experiences and advocate for social change. Social media has also played a crucial role in organizing protests and movements, bringing attention to important social and political issues.\n",
        "\n",
        "However, the rise of social media has also brought about concerns regarding privacy. Users often share personal information online, which can be exploited by malicious entities. Social media platforms have faced criticism for their handling of user data and breaches of privacy. It is crucial for individuals to be cautious about the information they share and to understand the privacy settings offered by these platforms.\n",
        "\n",
        "Another significant concern associated with social media is its impact on mental health. The constant exposure to curated and idealized representations of others' lives can lead to feelings of inadequacy and low self-esteem. Social media platforms have also been linked to increased rates of anxiety, depression, and cyberbullying. It is essential for individuals to use social media mindfully and seek support when needed.\n",
        "\n",
        "Furthermore, the spread of misinformation on social media has become a pressing issue. False information can easily go viral and influence public opinion. This has serious implications for democracy, public health, and social cohesion. It is important for users to critically evaluate the information they encounter and for platforms to take responsibility in combating the spread of misinformation through fact-checking and content moderation.\n",
        "\n",
        "In conclusion, social media has had a profound impact on society. It has connected people across borders, facilitated the exchange of information, and empowered marginalized communities. However, it has also raised concerns about privacy, mental health, and the spread of misinformation. It is essential for individuals, platforms, and policymakers to work together to harness the positive aspects of social media while addressing its challenges. By promoting responsible use, protecting user privacy, and ensuring the reliability of information, we can maximize the benefits of social media and create a more inclusive and informed society.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tokens = sorted(set(dataset))\n",
        "print(unique_tokens[:10],len(unique_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtYs3YwjLldx",
        "outputId": "2017c5c5-23c0-40ed-c279-1935ee379614"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', \"'\", ',', '-', '.', '0', '4'] 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create encoder and decoder\n",
        "stoi = {ch : i for i,ch in enumerate(unique_tokens)}\n",
        "itos = {i: ch for i,ch in enumerate(unique_tokens)}\n",
        "encoder = lambda string : [stoi[x] for x in string]\n",
        "decoder = lambda vector : [itos[x] for x in vector]"
      ],
      "metadata": {
        "id": "6EQ7HvkHL3FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_part = encoder(dataset[:10])\n",
        "encoded_part"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IZqSAbpLy9Q",
        "outputId": "f0ab088b-1e43-4a14-a02c-0ff7c7d477dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 20, 43, 40, 28, 2, 1, 14, 28, 40]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_part = decoder(encoded_part)\n",
        "\"\".join(decoded_part)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UM5VyAR3MtmZ",
        "outputId": "2aeb0f60-e7e7-49f1-b3fe-b8861009cfea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nSure! Her'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_dataset = encoder(dataset)\n",
        "train_dataset = encoded_dataset[:int(len(encoded_dataset)*.8)]\n",
        "test_dataset = encoded_dataset[int(len(encoded_dataset)*.8):]\n",
        "print(len(train_dataset),len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP8PZWRDM8a8",
        "outputId": "9fad4aec-8973-4adb-8989-a8ba933451d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2691 673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.randint?"
      ],
      "metadata": {
        "id": "6CXBMSBQQW0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now we need to split dataset into batches and sequences (time dimention)\n",
        "## create random starting points for batch\n",
        "def generate_batch(dataset,batch_size = 32,seq_len = 8):\n",
        "  random_ids = np.random.randint(len(dataset) - seq_len - 1,size= (batch_size,))\n",
        "  ## stack creates a new 0th axis\n",
        "  x = np.stack([dataset[i:i + seq_len] for i in random_ids])\n",
        "  y = np.stack([dataset[i+1:i+seq_len + 1] for i in random_ids])\n",
        "  # context, target pairs\n",
        "  return torch.tensor(x),torch.tensor(y)"
      ],
      "metadata": {
        "id": "t-4AUuOONuE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_batch(train_dataset,4,8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WHo0XlQP70s",
        "outputId": "5f0a1c6c-366b-4f06-ae39-ead0a5af4fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[40, 34, 27,  7,  1, 19, 34, 24],\n",
              "         [15, 42,  1, 32, 41,  1, 26, 40],\n",
              "         [28, 46, 38, 37, 41, 43, 40, 28],\n",
              "         [31, 28,  1, 41, 38, 40, 28, 24]]),\n",
              " tensor([[34, 27,  7,  1, 19, 34, 24, 42],\n",
              "         [42,  1, 32, 41,  1, 26, 40, 43],\n",
              "         [46, 38, 37, 41, 43, 40, 28,  1],\n",
              "         [28,  1, 41, 38, 40, 28, 24, 27]]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now we have found batch for our work\n",
        "## remainig part is to create dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNr4SiqDQAwg",
        "outputId": "320ccdef-6b70-42a0-b112-d12879bc15a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f73d65bdb30>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## defining some length of vectors\n",
        "vocab_size = len(unique_tokens)\n",
        "seq_len = 8\n",
        "feature_len = 32\n",
        "head_size = 16\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "uP7r_1vJV1i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "pCMlbLa3_gZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
        "  def forward(self,idx,target = None):\n",
        "    ## idx and target are batch of sequences(tokens) -> B T\n",
        "    ## after being employed over embedding -> B * T * C\n",
        "    ## so embedding works on sequence of\n",
        "    logits = self.token_embedding_table(idx)\n",
        "    if target ==  None:\n",
        "      return logits\n",
        "    ## now categorical cross entropy excepts [vector] [singleoutput] as input\n",
        "    ## that means only 2d 1 for batch other for [vector] or [singleOutput](1 dimention)\n",
        "    ## so need to convert this dataset in desired format\n",
        "    B,T,C = logits.shape\n",
        "    logits = logits.view(-1,C)\n",
        "    target = target.view(-1)\n",
        "    score =criterion(logits,target)\n",
        "\n",
        "    return logits,score\n",
        "  def generator(self,idx,new_max_tokens):\n",
        "    for _ in range(new_max_tokens):\n",
        "      logits = self(idx) # batch token C\n",
        "      logits = logits[:,-1,:] # we needs batch x C of final dimention only\n",
        "      probs = f.softmax(logits,dim = -1) ## only B X C\n",
        "      predictions = torch.multinomial(probs,num_samples=1)  ## B X 1 output\n",
        "      idx = torch.cat([idx,predictions],axis = 1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "2Xlw6GrgSl5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## some tests\n",
        "idx = np.array(test_dataset[:1]) ## it takes all dimention\n",
        "idx2 = np.array(test_dataset[1]) ## this ohmits this dimention as we are taking single element\n",
        "idx.shape,idx2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4toxoVVDpDD",
        "outputId": "f04df9d8-b5eb-415d-aea3-7efdac182ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1,), ())"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([[1,2],[1,2],[1,2]])\n",
        "## now depending on which data we are adding to it\n",
        "new_data = torch.tensor([[2,3]])\n",
        "x.shape,new_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L62_8f4kCOd4",
        "outputId": "c9281472-2e64-4f69-a56c-1cef51605bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 2]), torch.Size([1, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cat([x,new_data],axis = 0) ### concatenation should pass only axis on which data differs\n",
        "### that means to concatenate (3,2) (1,2) axis = 0 is the different one other's are same and hence can be concatenated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmAoLd9IDAsj",
        "outputId": "45ccf7d0-8412-4a2b-88db-ae7c7795ac75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2],\n",
              "        [1, 2],\n",
              "        [1, 2],\n",
              "        [2, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dx,dy = generate_batch(train_dataset,8,1)"
      ],
      "metadata": {
        "id": "Wc1JAbZvV_RL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dx.shape,dy.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyvbngV0-p7U",
        "outputId": "07232342-3ea6-4937-cea7-ab2dc6b2b859"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 1]), torch.Size([8, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## generation output is for each batch ## take 1 batch we are left with T dimention now decode it"
      ],
      "metadata": {
        "id": "LOujyoRqDpcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = BigramLanguageModel(vocab_size)"
      ],
      "metadata": {
        "id": "J4ik2H_3Vv6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits,loss = m(dx,dy)\n",
        "print(loss.item()) ## using item over loss gives value we are working with"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp-KUWwHWTXC",
        "outputId": "0794e344-5405-4f76-9b29-15ff74fa8f14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.623099327087402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## let's just say 0 being the first token\n",
        "tokens = m.generator(torch.tensor([[35]]),200)[0]\n",
        "print(\"\".join(decoder(tokens.tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8U54aJlfFz-H",
        "outputId": "32adf55b-2046-46f1-cc07-f19e2e62a797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "msxhg,IM,\"UflcaP4mc4w''oMWO'\"yySwF\"yFUTuF0M'vzBNU,.l.A4u\n",
            "TnIiIsxtufz:.aTO4mwbx:WfcA\"v,.r!AUxvnIBetnhvl.N\"vIAOlIrxfTN\"AuctBOMk.o0fWlin-WkaPAerrz0qBeoFUpTThgAIdx'UxhMmfiFd:Iy.\"ofdq:wepmrug-co0fev0lOzAnhm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now let's train bigram model\n",
        "optimizer = optim.AdamW(params = m.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "iTkTEbFxHHlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10000\n",
        "for i in range(num_epochs):\n",
        "  x,y = generate_batch(train_dataset)\n",
        "  logits,loss = m(x,y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if(i%500 == 0):\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miiWZoxhHHnq",
        "outputId": "e8b40be2-af89-40ec-9c62-42874a3a2341"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.454430103302002\n",
            "3.8837852478027344\n",
            "3.4051735401153564\n",
            "2.9932796955108643\n",
            "2.839930534362793\n",
            "2.663543939590454\n",
            "2.6118483543395996\n",
            "2.4756112098693848\n",
            "2.4258763790130615\n",
            "2.3992905616760254\n",
            "2.314478635787964\n",
            "2.313333511352539\n",
            "2.2423791885375977\n",
            "2.266751766204834\n",
            "2.353645086288452\n",
            "2.299842119216919\n",
            "2.304314374923706\n",
            "2.2036361694335938\n",
            "2.149061918258667\n",
            "2.161999225616455\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = m.generator(torch.tensor([[5]]),200)[0]\n",
        "print(\"\".join(decoder(tokens.tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAbJ38QcHHp-",
        "outputId": "2d60c88d-a1c9-413b-adce-3173c9f6c375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ", ake!Soninfonen ly. w bovein of icy on ivericeas crendusoofrid be! cciay tioSocin harecAn fripraneso s o owhad exprmpd n Imeas ofoct pldactererentiarsoualted thed updsedor wial eth Mo wa gediverenghex\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mathematical trick towards attention"
      ],
      "metadata": {
        "id": "xx2PKQ209gfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## attention is taking previous context in account while making next work prediction\n",
        "## so this must be decided by current word that how much of previous word matters here\n",
        "## also since this is just about prediction we need to take previous words in account we can't have next words\n",
        "## in works like sentimental analysis we can definitely use something like attention with previous<>next words in account\n",
        "## so to create attention we first need to take previous data and then average their context vector or data value whatever way previous info stored as\n",
        "sample_data = torch.rand((3,4,5)) ## 3 batches each having 4 tokens represented as vector of 5 elements\n",
        "context_data = torch.zeros((3,4,5))\n",
        "for i in range(sample_data.shape[0]) :\n",
        "  for j in range(sample_data.shape[1]) :\n",
        "    context = sample_data[i,:j+1] # TxC\n",
        "    context_data[i][j] = torch.mean(context,axis = 0) ## mean along row T,\n",
        "sample_data[0],context_data[0]"
      ],
      "metadata": {
        "id": "G__viXj8WY-e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2f3514d-ef40-4b14-e90c-14daa0613dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.1080, 0.8708, 0.3333, 0.1221, 0.1974],\n",
              "         [0.5288, 0.5014, 0.3724, 0.8378, 0.8934],\n",
              "         [0.8923, 0.5049, 0.5240, 0.2993, 0.0722],\n",
              "         [0.1155, 0.6658, 0.4625, 0.0577, 0.1422]]),\n",
              " tensor([[0.1080, 0.8708, 0.3333, 0.1221, 0.1974],\n",
              "         [0.3184, 0.6861, 0.3529, 0.4799, 0.5454],\n",
              "         [0.5097, 0.6257, 0.4099, 0.4197, 0.3877],\n",
              "         [0.4111, 0.6357, 0.4231, 0.3292, 0.3263]]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## doing attention work using matrix multiplication\n",
        "## we want for each T decide which T should it take from that batch\n",
        "B,T,C = sample_data.shape\n",
        "wei = torch.ones(T,T)\n",
        "wei = torch.tril(wei) ## making it lower triangular matrix\n",
        "wei = wei/torch.sum(wei,dim = 1,keepdim=True) ## all elements in jth dimention are added which is then used to divide\n",
        "context_data = wei@sample_data\n",
        "sample_data[0],context_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bQrLT4zitp2",
        "outputId": "66a9c695-9154-4b39-9d8c-19f0af35c1ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.1080, 0.8708, 0.3333, 0.1221, 0.1974],\n",
              "         [0.5288, 0.5014, 0.3724, 0.8378, 0.8934],\n",
              "         [0.8923, 0.5049, 0.5240, 0.2993, 0.0722],\n",
              "         [0.1155, 0.6658, 0.4625, 0.0577, 0.1422]]),\n",
              " tensor([[0.1080, 0.8708, 0.3333, 0.1221, 0.1974],\n",
              "         [0.3184, 0.6861, 0.3529, 0.4799, 0.5454],\n",
              "         [0.5097, 0.6257, 0.4099, 0.4197, 0.3877],\n",
              "         [0.4111, 0.6357, 0.4231, 0.3292, 0.3263]]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now we need Affinity vector decides how much of previous data we need to use\n",
        "## for that purpose we need to give everything some weights and normalize those weights properly\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill((tril == 0),float('-inf'))\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9xw094DkHes",
        "outputId": "abdf18c4-f7d8-477c-a542-c8d7fca013c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = f.softmax(wei,dim = 1)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItqvYxzkzZO",
        "outputId": "764672b5-187f-4184-f0ac-01ff43d8cfc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_data = wei @ sample_data\n",
        "sample_data[0],context_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNA4NznNlJCZ",
        "outputId": "f0db4d24-74c0-430e-a791-40aaf76f5ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.1080, 0.8708, 0.3333, 0.1221, 0.1974],\n",
              "         [0.5288, 0.5014, 0.3724, 0.8378, 0.8934],\n",
              "         [0.8923, 0.5049, 0.5240, 0.2993, 0.0722],\n",
              "         [0.1155, 0.6658, 0.4625, 0.0577, 0.1422]]),\n",
              " tensor([[0.1080, 0.8708, 0.3333, 0.1221, 0.1974],\n",
              "         [0.3184, 0.6861, 0.3529, 0.4799, 0.5454],\n",
              "         [0.5097, 0.6257, 0.4099, 0.4197, 0.3877],\n",
              "         [0.4111, 0.6357, 0.4231, 0.3292, 0.3263]]))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### now affinity matrix (wei) needs to be created based on input token information\n",
        "## we'll used toke (size v) and get some query (what they wants to ask) and key (what do I stand for) for each token\n",
        "## after then when we do matrix multiplication if (requirement, standing) are similar they creates higher affinity\n",
        "## what model asks for and what is stand for is similar that will cause it to create higher affinity\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size,bias = False)\n",
        "query = nn.Linear(C,head_size,bias = False)\n",
        "k = key(sample_data)  ## B T 16\n",
        "q = query(sample_data) ## B T 16\n",
        "## now creating affinity matrix\n",
        "wei = q @ k.transpose(-2,-1) # B T T\n",
        "wei = f.softmax(wei,dim = -1)\n",
        "## now we also want to transform data, i.e. from data -> extract what it has to offer for which is then going to be concatenated\n",
        "value = nn.Linear(C,head_size,bias = False)\n",
        "x = value(sample_data)\n",
        "output = wei @ x  ## will product B T H dimention output\n",
        "output[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpwF-C2plmxw",
        "outputId": "f04587b4-e6b3-4f63-9ca7-c8628d9717ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head_size = 16\n",
        "n_embed = 32\n",
        "block_size = 8"
      ],
      "metadata": {
        "id": "zz6uaUIrqIoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## now creating head block\n",
        "class Head(nn.Module):\n",
        "  def __init__(self,n_embed = 32,head_size = 16): ## head_size defined globally making it more scalable\n",
        "    super().__init__()\n",
        "    self.query = nn.Linear(n_embed,head_size,bias = False)\n",
        "    self.value = nn.Linear(n_embed,head_size,bias = False)\n",
        "    self.key = nn.Linear(n_embed,head_size,bias = False)\n",
        "    self.register_buffer('tril',torch.tril(torch.ones(block_size,block_size)))\n",
        "  def forward(self,x):\n",
        "    ## tokens might differ in steps that's why we are creating it like this\n",
        "    B,T,C = x.shape\n",
        "    q = self.query(x)\n",
        "    k = self.key(x)\n",
        "    v = self.value(x)\n",
        "    wei = q @ k.transpose(-2,-1)*k.shape[-1]**-.5 ## multiplication will cause variance to be of order of head_size so need to normazlize\n",
        "    ## having such larger values softmax will oncverge to very large valules\n",
        "    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) ##avoiding communication with past\n",
        "    wei = f.softmax(wei,dim = -1) ## normalization of we matrix\n",
        "    return wei @ v  ## remember output is going to be B T head_size  ## this might need one more layer to convert it back to V dimention"
      ],
      "metadata": {
        "id": "kD7nfFwUpyWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HeadedBigramLanguageModel(nn.Module):\n",
        "  def __init__(self,vocab_size = 49,n_embed = 32,head_size = 16):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
        "    self.index_embedding_table = nn.Embedding(vocab_size,n_embed)\n",
        "    self.sa_head = Head(n_embed,head_size)\n",
        "    self.ln_head = nn.Linear(head_size,vocab_size) ## going from head -> vocab_size\n",
        "  def forward(self,idx,target = None):\n",
        "    ## idx and target are batch of sequences(tokens) -> B T\n",
        "    ## after being employed over embedding -> B * T * C\n",
        "    ## so embedding works on sequence of\n",
        "    token_embeds = self.token_embedding_table(idx)\n",
        "    index_embeds = self.index_embedding_table(torch.arange(idx.shape[-1])) ## at every batch each T size values are going to get their C size index information\n",
        "    x = token_embeds*index_embeds ## should auto extend it's dimention over batch as it's element wise multiplication\n",
        "\n",
        "    x = self.sa_head(x)\n",
        "    logits = self.ln_head(x)\n",
        "    if target ==  None:\n",
        "      loss = None\n",
        "    else:\n",
        "      ## now categorical cross entropy excepts [vector] [singleoutput] as input\n",
        "      ## that means only 2d 1 for batch other for [vector] or [singleOutput](1 dimention)\n",
        "      ## so need to convert this dataset in desired format\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(-1,C)\n",
        "      target = target.view(-1)\n",
        "      score =criterion(logits,target)\n",
        "    return logits,score\n",
        "  def generator(self,idx,new_max_tokens):\n",
        "    for _ in range(new_max_tokens):\n",
        "      logits = self(idx) # batch token C\n",
        "      logits = logits[:,-block_size:,:] # don't take more than block size items as their context because trils are not of larger size here\n",
        "      probs = f.softmax(logits,dim = -1) ## only B X C\n",
        "      predictions = torch.multinomial(probs,num_samples=1)  ## B X 1 output\n",
        "      idx = torch.cat([idx,predictions],axis = 1)\n",
        "    return idx"
      ],
      "metadata": {
        "id": "CfyjBTWtsNz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cude' if torch.cuda.is_available() else 'cpu')\n",
        "hm = HeadedBigramLanguageModel(vocab_size,32,16)\n",
        "# hm.to(device)\n",
        "optimizer = optim.AdamW(params = hm.parameters(), lr = 1e-3)\n",
        "num_epochs = 10000\n",
        "for i in range(num_epochs):\n",
        "  x,y = generate_batch(train_dataset)\n",
        "  # x.to(device)\n",
        "  # y.to(device)\n",
        "  logits,loss = hm(x,y)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if(i%500 == 0):\n",
        "    print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51WazwYUuGTJ",
        "outputId": "2978aa37-e18a-49d4-ecf3-202f554898ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.9133803844451904\n",
            "2.7331979274749756\n",
            "2.5434930324554443\n",
            "2.4078478813171387\n",
            "2.4238455295562744\n",
            "2.1879477500915527\n",
            "2.198587656021118\n",
            "2.2066116333007812\n",
            "2.1335368156433105\n",
            "2.182493209838867\n",
            "2.0280001163482666\n",
            "2.0662906169891357\n",
            "2.0876331329345703\n",
            "2.0656802654266357\n",
            "2.145443916320801\n",
            "2.168588876724243\n",
            "2.1405746936798096\n",
            "2.2180380821228027\n",
            "2.016458749771118\n",
            "2.0461058616638184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### now single head is not enough we need to have many head to make things good\n",
        "### also after we have selected values using attention we also need to process that information\n",
        "### so creating multiheaded layer and forward layer\n",
        "\n",
        "\n",
        "### some more advancements we need some regularizers and projections layers to multiheads,feedforward also some dropouts for these linear layers required\n",
        "### one more thing add resudual networks by making x = x + layer(x) that will create residual connections\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "  def __init__(self,head_size = 16,num_heads = 4,n_embed = 32): # head_size is global parameter\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(n_embed,head_size//num_heads) for _ in range(num_heads)])  ## might need to change heads definition to take head_size as input\n",
        "    self.projection = nn.Linear(head_size,num_heads*head_size)\n",
        "    self.drop = nn.Dropout(.4)\n",
        "  def forward(self,x):\n",
        "    x =  torch.cat([head(x) for head in self.heads],axis = -1) # x -> B T C(head_size)\n",
        "    x = nn.ReLU(self.projection(x))\n",
        "    x = self.projection(x)\n",
        "    x = nn.ReLU(x)\n",
        "    x = self.drop(x)\n",
        "\n",
        "\n",
        "## for now we'll change feedFowards instead, this is to take head_size -> n_embed\n",
        "class FeedForward(nn.Mudule):\n",
        "  def __init__(self,head_size = 16,n_embed = 32):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(head_size,n_embed),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.net(x)\n",
        "class Block(nn.Module): ## attention + feedFoward\n",
        "  def __init__(self,n_embed = 32,head_size = 16,n_heads = 4):\n",
        "    super().__init__()\n",
        "    self.sa = MultiHeadedAttention(head_size,n_heads)  # self attention\n",
        "    self.ffwd = FeedForward(head_size,n_embed)  # feed forward\n",
        "  def forward(self,x):\n",
        "    return self.ffwd(self.sa(x))\n"
      ],
      "metadata": {
        "id": "kX-ERy2hv1YT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "3fd82574-3e04-4457-9dd1-48e53cbda635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-917ed486792b>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    def __init__(self,head_size = 16,n_embed = 32)\u001b[0m\n\u001b[0m                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## final model creation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCsYzEQjamCF",
        "outputId": "3cc8f6fb-ec3a-4ed7-be3d-cc132f4b51f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fd1eb5e92d0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIDN0vPq7f-0",
        "outputId": "10410b5d-6a19-4676-a65b-8e13f3873e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-09 09:14:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-07-09 09:14:30 (173 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "B6wPS87v4ot4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\"\n",
        "    one head of self-attention\n",
        "    this also requires dropouts in attention matrix\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ],
      "metadata": {
        "id": "3QnQLOc74pIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "Uotij5qa4pKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity\n",
        "        this is to give thinking time to data extracted by attention\n",
        "        it first maps parameters to 4 times then gets them back to origial size\n",
        "        this also uses dropouts to this higher linear layers\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "IlczQgIS4pM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation\n",
        "        this will also add some residual connections to network with layer normalization\n",
        "        this is required while connecting many layers together\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "NnPU3UaV4pPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "    \"\"\"\n",
        "    this is just final collection of all models\n",
        "    we are using embeddings (position,token)\n",
        "    then we multiply both these embeddings\n",
        "    then uses many layers of blocks\n",
        "    finally a linear head to give output of vocab_size\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "bkWxo8LV4pSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPzNLvek7OvR",
        "outputId": "13136cd2-eaeb-4cfc-d2c0-c965a8d4c8d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.3759, val loss 4.3710\n",
            "step 500: train loss 1.7478, val loss 1.9022\n",
            "step 1000: train loss 1.4023, val loss 1.6221\n",
            "step 1500: train loss 1.2712, val loss 1.5413\n",
            "step 2000: train loss 1.1937, val loss 1.5184\n",
            "step 2500: train loss 1.1288, val loss 1.4963\n",
            "step 3000: train loss 1.0765, val loss 1.4904\n",
            "step 3500: train loss 1.0226, val loss 1.5102\n",
            "step 4000: train loss 0.9695, val loss 1.5283\n",
            "step 4500: train loss 0.9142, val loss 1.5485\n",
            "step 4999: train loss 0.8660, val loss 1.5774\n",
            "\n",
            "But with price-pitch gentlewise-cowaning kiss;\n",
            "And Capulet, alwo'd with flesh sun\n",
            "Arrows charges the officet o' the general;\n",
            "And yet no more than commen all to him all.\n",
            "\n",
            "BRAKENBURY:\n",
            "How comes here come along Warwick! what will be the day\n",
            "To ven yieldest then of figure to groan!\n",
            "I could know could to provil to our drink?\n",
            "\n",
            "LADY ANNE:\n",
            "Would in the Angelo?\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Faith, look to no seeky children,\n",
            "I will not try her heart with this city blood,\n",
            "In Edward sesting in receiving report.\n",
            "\n",
            "YORK:\n",
            "Ha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4OD7kiB7ook"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}